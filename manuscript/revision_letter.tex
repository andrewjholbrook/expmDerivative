%\documentclass[standard,
%               backaddress=off,
%               foldmarks=false,
%               enlargefirstpage,
%               pagenumber=off,
%               parskip=half
%               ]{scrlttr2}
%
%\setkomavar{fromname}{Gabriel Hassler}
%\setkomavar{fromaddress}{Department of Computational Medicine / UCLA \\ 5303 Life Sciences \\ Box 951766 \\ Los Angeles, CA 90095}
%\setkomavar{fromphone}{(202) 557-5140}
%\setkomavar{fromemail}{ghassler@g.ucla.edu}
%%\setkomavar{department}{Department of Computational Medicine}


\documentclass[12pt]{article}

\usepackage{amssymb}
\usepackage{geometry}
\usepackage{url}
\geometry{letterpaper} % or letterpaper (US) or a5paper or....
\geometry{margin=1in} % for example, change the margins to 2 inches all round
\usepackage[table]{xcolor}

\newenvironment{reply}{$\triangleright$\bfseries}{$\triangleleft$}
\renewenvironment{quote}
               {\list{}{\rightmargin\leftmargin}%
                \item\relax\normalfont}
               {\endlist}

%
%\newcommand{\bibsection}[1]{}
%\newcommand{\section}[1]{}
%\newcommand{\newblock}{}
%
%   \newenvironment{thebibliography}[1]%
%      {References\begin{description}}{\end{description}}
%   \newcommand{\htmlbibitem}[2]{\label{#2}\item[{[#1]}]}
\usepackage{natbib}
\usepackage[colorlinks, citecolor={blue}]{hyperref}
\usepackage{amsmath}
\usepackage{float}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{setspace}
\usepackage{xr}
\usepackage[textsize=tiny]{todonotes}
\usepackage{algorithm,algorithmicx,algpseudocode}
%\usepackage{subfigure}
\setlength{\parindent}{0pt}
\setlength{\parskip}{12pt}

\usepackage{braket}


\usepackage{amsthm}

\definecolor{trevorblue}{rgb}{0.330, 0.484, 0.828}
\definecolor{trevoryellow}{rgb}{0.829, 0.680, 0.306}


\def\suppDoc{pnas_revision}
\externaldocument{\suppDoc}
%\input{missing_traits_supplement_revision.xtr}
%\input{revision_values.sty}
%\input{new_floats.tex}

\usepackage{makecell}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{bm}

\usepackage{todonotes}

\usepackage[format=plain,
labelfont={it,bf},
textfont=it]{caption}

\newcommand{\mb}{\mathbf}
\newcommand{\mc}{\mathcal}
\newcommand{\dx}{\mbox{d}}

\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand*{\fplus}{\genfrac{}{}{0pt}{}{}{+}}
\newcommand*{\fdots}{\genfrac{}{}{0pt}{}{}{\cdots}}


\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\numTaxa}{N}
\newcommand{\numTraits}{D}
\newcommand{\numDatasets}{M}
%\newcommand{\numLatent}{D}
\newcommand{\taxonIndex}{i}
\newcommand{\traitIndex}{j}
\newcommand{\traitData}{\vec{Y}}
\newcommand{\traitDatum}{y}
\newcommand{\datasetIndex}{m}
\newcommand{\exemplar}{\text{e}}

\newcommand{\sequences}{\vec{S}}
\newcommand{\latentData}{\vec{X}}
\newcommand{\latentdata}{\vec{z}}
\newcommand{\latentDatum}{x}
\newcommand{\phylogeneticParameters}{\boldsymbol{\phi}}
\newcommand{\phylogeny}{{\cal G}}
\newcommand{\tree}{\phylogeny}
%\newcommand{\otherParameters}{\boldsymbol{\
\newcommand{\transpose}{^{t}}

\newcommand{\distanceMatrix}{\mathbf{Y}}
\newcommand{\distance}{y}
\newcommand{\summant}{r}



\newcommand{\cdensity}[2]{\ensuremath{p(#1 \,|\,#2)}}
\newcommand{\density}[1]{\ensuremath{p(#1 )}}

\newcommand{\treeNode}{\nu}

\newcommand{\traitVariance}{\mathbf{\Sigma}}
\newcommand{\nodeIndex}{c}
%\newcommand{\parent}[1]{\mbox{\tiny pa}(#1)}
\newcommand{\parentBig}[1]{\mbox{pa}(#1)}

\newcommand{\sibling}[1]{\mbox{\tiny sib}(#1)}
\newcommand{\siblingBig}[1]{\mbox{sib}(#1)}

\newcommand{\rootMean}{\boldsymbol{\mu}_0}
\newcommand{\rootVarianceScalar}{\tau_0}
\newcommand{\unsequencedVarianceScalar}{\tau_{\exemplar}}
\newcommand{\treeVariance}{\vec{V}_{\tree}}
\newcommand{\hatTreeVariance}{\hat{\vec{V}}_{\tree}}
\newcommand{\mdsSD}{\sigma}
\newcommand{\mdsVariance}{\mdsSD^2}
\newcommand{\residual}{\hat{\traitDatum}}
\newcommand{\modelDistance}{\delta}
\newcommand{\cdf}{\phi}
\newcommand{\normalCDF}[1]{\Phi \left( #1 \right)}

\newcommand{\order}[1]{{\cal O}\hspace{-0.2em}\left( #1 \right)}

\newcommand{\rootNode}{\nu^{\datasetIndex}_{2 \numTaxa_{\datasetIndex} -1 }}
\newcommand{\pathLength}[1]{d(F, #1 )}
\newcommand{\pathLengthNew}[2]{
	d_{F}
	(
	{#1}, {#2}
	)
}
\newcommand{\J}{\vec{J}}
\newcommand{\pprime}{^{\prime}}
\newcommand{\otherIndex}{i \pprime}
\def\kronecker{\raisebox{1pt}{\ensuremath{\:\otimes\:}}}

\newcommand{\x}{\mathbf{x}}

%\graphicspath{{../../Graphs/}}

\def\journalName{Journal of Multivariate Analysis}
\def\journalAbbr{JMVA}
\def\editor{Hongyu Zhao}
\def\editorTitle{Theory and Methods Co-Editor}
\def\editorLast{Zhao}
\def\pubOrg{American Statistical Association}
\def\pubAddressOne{732 North Washington Street}
\def\pubAddressTwo{Alexandria, VA 22314-1943}

\def\paperTitle{On the surprising effectiveness of a simple matrix exponential derivative approximation, with application to global SARS-CoV-2}
\def\paperID{MS\# 2023-10809}

%\newcommand{\bigo}[1]{\cal{O}\hspace{-.5mm}\left(#1\right)}
\onehalfspacing

\renewcommand{\x}{\mathbf{x}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\TTheta}{\boldsymbol{\theta}}
\newcommand{\TTTheta}{\boldsymbol{\Theta}}
\newcommand{\tr}{\mbox{tr}}
\newcommand{\X}{\mathbf{X}}

\usepackage{tikz}
\usetikzlibrary{matrix}
\usetikzlibrary{backgrounds}
\usetikzlibrary{calc}
\usetikzlibrary{arrows,shapes}
\usetikzlibrary{decorations}
\usetikzlibrary{decorations.pathmorphing}
\usetikzlibrary{fit}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{shapes.misc}
\usetikzlibrary{shapes.geometric}

\newtheorem{thm}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{cor}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{mydef}{Definition}
\newtheorem*{assumption}{Assumption}
\newtheorem{clm}{Claim}

\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\GD}[1]{#1}
\newcommand{\GDcomment}[2][purple]{GD: \textcolor{#1}{#2}}


\newcommand{\y}{\mathbf{y}}
\newcommand{\QQ}{\mathbf{Q}}
\newcommand{\LLambda}{\boldsymbol{\Lambda}}
\newcommand{\M}{\mathbf{M}}
\newcommand{\JJ}{\mathbf{J}}
\newcommand{\II}{\mathbf{I}}
\newcommand{\AAA}{\mathbf{A}}

\newcommand{\Zero}{\boldsymbol{0}}
\newcommand{\ttheta}{\boldsymbol{\theta}}
\newcommand{\mom}{\boldsymbol{\xi}}
\newcommand{\ppi}{\boldsymbol{\pi}}
\renewcommand{\P}{\mathbf{P}}
\newcommand{\p}{\mathbf{p}}
\newcommand{\q}{\mathbf{q}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\Z}{\mathbf{E}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\TT}{\mathbf{T}}
\newcommand{\RM}{\mathbf{R}}
\newcommand{\UU}{\mathbf{U}}
\newcommand{\VV}{\mathbf{V}}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\NN}{\mathbb{N}}





\begin{document}

\input{pnas_revision.xtr}

\centerline{\large \bf Reponse to Editor and Reviewer Comments}

To the Editor and Reviewers:

On behalf of my coauthors, I thank you for your thoughtful recommendations and the opportunity to revise our manuscript ``\paperTitle" (\paperID).
We have carefully considered your helpful suggestions and have addressed them to the best of our abilities.  We believe that the manuscript is now significantly improved and are thankful for the time and energy you have put forth.

In particular, we have:
\begin{itemize}
	\item clarified how the present work relates to our other work that includes thorough simulation studies \citet{magee2023random} and provides strong theoretical guarantees for surrogate-trajectory HMC \citet{glatt2020accept};
	
	\item implemented an additional simulation study which compares accuracy for multiple algorithms, different model sizes and different signal strength regimes;
    
    \item expanded upon the SARS-CoV-2 application, relating it to previously established results and improving a key figure;
	
	\item responded to all other Editor/Reviewer questions and comments.
\end{itemize}


The document below contains my replies (in \textbf{bold}) alongside the Editor's and Reviewers' comments (normal text).  I list changes to the text with indented normal text set off by quotation marks.

With sincere gratitude,

Andrew Holbrook
\clearpage


{\Large \bf Editor and Reviewer Comments:}


\section*{Editor}

Dear Andrew, 

Two reviews of your manuscript are somewhat split, but they also focus on different aspects of your paper. So the paper cannot be accepted. Rev 2 addresses the mathematical dimension of your study (as well as the awkward juxtaposition of the mathematical presentation and the data example); Rev 1 seems to focus on details of the covid data but also has some queries about the methods. The paper is well outside anything to do with my expertise, but I can certainly identify with the general points of Rev 2, especially the absence of demonstrated method performance on data where you know the answer. 

If you think you can revise to overcome reviewer concerns, you are welcome to do so. Any revision will be subjected to re-review with no prejudice toward acceptance. I think that both reviewers are highly appropriate, and given that their expertise far exceeds mine, I cannot apprise you of whether a successful revision is feasible. 

Jim 

\begin{reply}
	Thank you for the opportunity to submit a revised manuscript. We have made ample additions to the manuscript in response to the suggestions made by both reviewers. Among the many improvements we have made to this manuscript, the simulation study suggested by Reviewer 2 helps the paper smoothly transition between theory and application.
\end{reply}


\section*{Reviewer 1}

Phylogeography inference has been a very difficult problem in the sense of the tree size and the dimensionality of states. However, in this manuscript, the authors have shown that, based on the CTMC model, the first-order approximation of the gradient towards the rate matrix can improve the computational efficiency with acceptably bounded errors, which can be hugely confined with correction. The method is applied to SARS-CoV-2 transmission to infer the traveling factors across 33 regions, framed in a mixed-effects model. I am not an expert in Graph Theory and Random Matrix Theory but the manuscript is clear enough for me to understand. I, therefore, am strongly recommending for publication in PNAS, while there are still a few issues that the authors need to be addressed properly.

\begin{reply}
	Thank you for taking the time to review our manuscript and for your positive reception of our work.  We have carefully addressed each of your helpful recommendations in our revised manuscript, which is now greatly improved as a result.  Please find our responses listed below.
\end{reply}

1. The authors spend 3 out of 7 pages in the main text and three corresponding sections in the Appendix discussing the refined error and the proof, while postpone the application with the affine-corrected error to future work. I would expect more than a relatively abstract error comparison figure for this issue. For example, a simple numerical simulation similar to the last paragraph in Application to compute the posterior likelihood ratio or posterior density ratio of the corrected and the uncorrected methods, because the authors eventually applied the method within a Bayesian framework.


\begin{reply}
TBD (Simulation related)
\end{reply}

2. I am curious about the selection of the extra two additional Chinese provinces and 11 additional countries when fitting the mixed-effects model and how the inclusion of the extra regions improve the fitting. Is there any criteria to determine the inclusion?


\begin{reply}
TBD (Simulation related)

Recent work \citep{gotovos2021scaling} suggests that inclusion of auxiliary states can lead to improved model fit even when these additional states do not correspond to observed data.  In future analyses, we aim to include hundreds of global regions.  For the time being, we have incorporated this question into the simulation study...
\end{reply}

3. For the mixed-effects model, do the authors assume independent predictors for the fixed effects? It seems to me that, in Fig. 2, the air traffic proximity and the intracontinental proximity are highly correlated, which can be the reason why $\theta_2$ is not statistically significant. Then what’s the justification for the predictor selection?


\begin{reply}
Multiple regression models (and extensions) do not assume independence between fixed-effect predictors, but the interpretations of the individual regression coefficients do need to be adjusted accordingly. We must interpret each association holding all other predictors fixed.  Yes, intracontinental proximity is no longer statistically significant, but the positive association it does maintain has a stronger interpretation because we are accounting for the role played by air traffic proximity.    If anything, we would like to include larger numbers of fixed-effect predictors---such as google mobility flows \citep{lemey2021untangling}---in future analyses.
\end{reply}

4. Am I correct that the total number of particles (i.e., samples) in the MCMC framework is 2,000? The ESS for $\tau$ is extremely small, do the authors have any explanations?


\begin{reply}
We generate 8 million samples but throw away the first 10\% (80,000) as burnin and save only 1 sample out of every 10,000.  Such thinning does not increase ESS, but it helps with memory constraints.  After removing burnin and thinning, we have 721 samples.   The ESS of 185 for $\tau$ is not wonderful but not bad.  Having fit these Bayesian bridge models in a number of different contexts, we have come to expect $\tau$ to be difficult.   Hierarchical hyperparameters mix more poorly than those that are close to the data, and $\tau$ is a global hyperparameter that dictates the distribution of a large number of random-effects that stand between it and the data.
\end{reply}

5. What does the random effect matrix look like? The authors indicate that air traffic proximity is the only significant fixed-effect predict, then I would wonder how much does the random effect contribute to the model?




\setcounter{figure}{4}
\begin{figure}[t]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/randEffects.pdf}
	\caption{Posterior means for exponentiated random effects convey expected multiplicative deviations from the portion of the rate attributable to fixed effects for each corresponding element of the generator matrix.  Notably, we infer a roughly 1.81-fold posterior mean increase in the rate of transitions from the US to Hubei, CN, beyond that portion of the rate which may be explained by fixed effects.  Less pronounced are posterior mean multiplicative increases of 1.34 (US to Italy), 1.27 (US to UK), 1.44 (Netherlands to Italy) and 1.21 (Guangdong, CN, to Hubei, CN).}%\label{fig:randEffs}
\end{figure}

\begin{reply}
This is a good question.  Figure \ref{fig:randEffs} show posterior means of the exponentiated random effects, which amount to multiplicative deviations from the fixed-effect model.  As one might expect, only a small number of random effects deviate from 0, but those that do deviate are highly suggestive.  We stop short of interpreting exactly why the transition from the US to Hubei, CN, exhibits largest deviation (a 1.81-fold increase).  We have added this figure and the following short discussion to the appendix:
	\begin{quote}
		Figure \ref{fig:randEffs} displays posterior means of the exponentiated random effects, which one may interpret as multiplicative deviations from the fixed effects' contributions to the generator matrix.  Whereas the vast majority of exponentiated random effects have posterior means close to 1, indicating no deviation from the fixed-effect model, a few exhibit posterior means that are significantly greater than 1.  In particular, the rate element corresponding to transfer from the US to Hubei, CN, exhibits a 1.81-fold random-effect derived increase to the fixed-effect component.  This large multiplicative increase agrees with, but goes beyond, the influence that the Hubei asymmetry holds for the entire generator matrix model. 
	\end{quote}	
\end{reply}

6. The ranges for predictor matrices and posterior mean rate matrix are different. However, the color palettes are identical, which makes it extremely hard to compare the impact of each predictor. For example, I cannot see how the sentence ``... but one may also see the influence of the Hubei asymmetry in, e.g., the squares corresponding to travel between Guangdong and Hubei provinces.'' makes sense visually from Fig. 2 and 3. It also doesn’t make sense to me conceptually, because air traffic proximity is the only significant predictor.


\begin{reply}
The ranges are different because the predictors are defined on the log scale, whereas the generator matrix is obtained by the (element-wise) exponentiation of the predictors times their respective regression coefficients plus the random-effects matrix.  Even though the Hubei asymmetry is not statistically significant (i.e., its 95\% credible interval contains 0), it still has a non-negligible effect size ($\approx 0.25$ posterior mean per Fig. 3a) that helps explain its appearance in Figure 3b.
\end{reply}

7.	The color palette in Fig. 4 makes it really hard to distinguish Hubei, other provinces in China, and other countries. It will be great if the authors can highlight Hubei in one color, other provinces in another color with gradients, and other countries in a third color with gradients. And why there are only 29 regions?


\setcounter{figure}{3}
\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/summary2.tree.pdf}
	\vspace{-0.5em}
	\caption{Posterior predictive modes for (unobserved) ancestral
		locations color a phylogenetic tree that describes the
		shared evolutionary history of 285 SARS-CoV-2
		samples.}\label{fig:tree}
\end{figure*}


\begin{reply}
We have updated Figure \ref{fig:tree} to the best of our ability to make the main geographic blocks visible.  The previous limitation to 29 regions was caused by software limitation that our updated figure also avoids.
\end{reply}


8.	The proposed method is able to include higher-dimension rate matrix compared with \citet{lemey2009bayesian,lemey2014unifying}. Can the authors then have an external comparison to the results in \citet{lemey2020accommodating} just visually to see whether the color pattern in the phylogenetic tree is matched?


\begin{reply}
	Thank you for the suggestion.  We now notice a strong correspondence between our updated Figure \ref{fig:tree} and Fig1 of \citet{lemey2020accommodating}.  We have accordingly added the following text to the manuscript:
	\begin{quote}
		After collapsing regions into 8 major blocks, Figure \ref{fig:tree} projects the empirical posterior predictive mode of these blocks onto the phylogenetic tree $\phylogeny$.  The general pattern looks similar to that of Figure 1 from \cite{lemey2020accommodating}, although the geographic blocking scheme differs slightly.
	\end{quote}
\end{reply}


\begin{reply}
	Again, we wish to thank you for taking the time to review our manuscript.  We are confident that the revised manuscript is much improved as a result of your efforts.
\end{reply}


\section*{Reviewer 2}

This is a review for the manuscript ``On the surprising effectiveness of a simple matrix exponential derivative approximation, with application to global SARS-CoV-2''. 
For the benefit of the authors on how they interpret my review, I am a theoretical/computational statistician with background on theoretical population genetics, interested in Markov chains and its implementations in Monte Carlo methods. 

\begin{reply}
	We thank you for lending your effort and expertise in reviewing our manuscript and for your helpful recommendations.  We have thoroughly implemented your suggestions and believe that the manuscript is greatly improved.  
\end{reply}

Before content-specific comments let me first clarify that there are issues with the numbering of theorems and remarks in the manuscript that I have received (e.g., theorem 2 is missing, and there are two remarks labeled as Remark 2), and below I stick to the numbering used in the manuscript that I have received for review to avoid any confusion. 

\begin{reply}
	We thank you for catching this numbering error, which we have addressed.  Now there are 3 theorems (1,2,3) and 5 remarks (1,2,3,4,5).
\end{reply}

The main idea in the manuscript revolves around the three theorems presented and how the approximations provided by these theorems improve computational efficiency in large matrices involved in the continuous time markov chains. The positives are: 1) The theorems are stated well as in solid applied mathematical work, 2) Their proofs seem to hold as far as I can tell. 
To me it is clear that if there are any gaps or minor incorrect statements they can be remedied with little effort and the theorems and corollaries will hold. It is also clear that a lot of effort and thought was put into these theorems and corollaries and I appreciate that, along with the clear exposition which made following the mathematical reasoning easy. 

\begin{reply}
	We thank you for your positive reception of our mathematical work.
\end{reply}

Unfortunately, the manuscript did not sit well with me, so to the negatives. There are two major negatives for me. The second item is the more important one, but the first motivates the second, and it deeply troubled me. 

1) I think the way the manuscript is positioned with respect to the interpretation of the scope of the work is inaccurate. After a first read of the manuscript, I was left with the impression that the work was originally conceived and intended as an applied mathematics or a stochastic process paper, which --for example-- with the omission of opening paragraph of the manuscript would stop before section 2. Presented this way, I think its scope is ideal for a specific technical subject-matter journal (e.g., applied math, stochastic process, statistics). With the inclusion of the first paragraph (which I do not find clear, see below specific comments on this), and section 2, I felt that there was an effort to make the paper appealing to a broader audience. I think this effort falls short of its aim and the integration of theoretical results with data looks ad-hoc. In particular, the example data set chosen to demonstrate the results does not have much information about the parameters, and it is as if it was included just because its dimensionality is large. This tells us nothing about the quality of inference (see 2 below) and also misleads us to believe that the approximation method presented is successful as a statistical computational --in the sense of methods that facilitate inference but they are not inferential methods themselves-- tool. I note that the example consists of a real data set in which we do not know whether the assumed probability model generating the data is true, let alone the true parameter values of that model. Even if there was a signal in the data, I would still be hesitant to accept the approximation method presented as a valid statistical computational tool in the context of MCMC, before it was well established with some simulation studies (again, see 2 below). Therefore, there is no way to assess the quality of inference about parameters with this example. The only use of this data set can be in supporting the claim that the approximation presented through the theorems provides a computational advantage at higher dimensions. But then, why use SARS-Cov-2 data for this goal, whereas it can be better established by a simulated data set? 


\begin{reply}
	We thank you for sharing your concerns, which we take seriously.  Before responding, let us point out that we have now added a simulation section (similar to that which you specify below) to the manuscript.   Now, we would like to argue that the decisions we made in structuring our manuscript are well-motivated based on our other work.
	\begin{enumerate}
		\item First, we have written a companion paper \citet{magee2023random} (minor revisions at Systematic Biology), which presents the basic approach, implements a thorough simulation study and applies the methodology to 3 real-world data analyses of varying dimensions and differing fixed-effect structures. 
		
		\item Second, when writing this paper, we had a clear understanding of the theoretical guarantees provided by surrogate-trajectory HMC.  Our paper \citet{glatt2020accept} (to appear at Annals of Applied Probability) proves that the surrogate-trajectory HMC transition kernel maintains detailed balance and that the algorithm samples from the target distribution with the same guarantees as vanilla HMC.  The results of \citet{glatt2020accept} hold for general state spaces, regardless of dimensionality.
	\end{enumerate}

Despite these theoretical guarantees, the method of \citet{magee2023random} performed even better than expected in terms of relatively high acceptance rates and relatively low autocorrelation between samples.  From our other previous work with surrogate-trajectory HMC \citep{li2019neural}, we knew this must suggest that the first-order matrix exponential gradient approximation must be pretty good.   
As a result, our goal in this paper was to find out what makes the first-order approximation so good and provide a salient example that illustrates the efficacy of the approximation in higher dimensions.  

In short, our original goal was not to prove validity of the general methodology, which we saw as already established through simulation studies, real-world analyses and measure theoretic arguments.   

Again, we thank you for sharing your concerns and hope that the broader context of our work helps explain the positioning of our manuscript.  Probably more importantly, we have updated our manuscript with a thorough simulation study (see below).
\end{reply}


2) To me, a huge barrier to accept that the approximations presented can be successful in aiding statistical inference is the lack of simulation studies to assess the quality of inference. (This is not about the type of simulations shown in Figure 1 as detailed below). For a work like this, before applying to real data, the first test would be to design a large simulation study, where the model and the parameters are known and to test whether the application of the approximation method when implemented within the Hamiltonian Monte Carlo has good statistical inference performance. MCMC makes its own approximations (mixing, convergence to stationarity, etc..) and errors introduced by an approximative method injected to an MCMC sampling might be exacerbated. Even the statistical performance of well-known MCMC algorithms can vary with different signal strengths (e.g., magnitude of the parameters relative to each other in a model), as well as the model size. It would be premature to think that a method is valid based solely on its computational performance (as it is presented in the manuscript) without first assessing its statistical performance of recovering an approximate joint posterior distribution of parameters and it passes rigorous tests. A way to check this is by simulating data from target distributions of interest under a variety of parameter value combinations and then apply the method to see whether the posterior distribution of parameters are recovered successfully (in the case of Bayesian inference for this manuscript). I am surprised that the authors did not see this as necessary especially because they are working with large mixed effect models where things can go awry pretty quickly. As an example, I would have liked to see a comparison of the statistical performance in say, a 3 x 3 simulation experiment design. Variable 1, The method with 3 levels: Standard Hamiltonian Monte Carlo, the naive method of reference (18), and the method of the manuscript; Variable 2, The model size with 3 levels: Small, Intermediate, Large; Variable 3, The number of parameters with considerable signal with 3 levels: 1 parameter, half of the parameters, all parameters in the model. 


\begin{reply}
Simulaiton. TBD.
\end{reply}

Specific Comments: 

* Please check numbering of theorems and remarks. 

\begin{reply}
	Again, we thank you for catching this error and have fixed the numbering accordingly. 
\end{reply}

* Regarding the opening paragraph. The first sentence did not make sense to me. What is big data (why is it in double quotes, anyways?), what is modeling schemes, what is structures. The whole paragraph is written vaguely and I felt dismissive of the applications of the work being presented. 

\begin{reply}
	It is clear that the opening sentence missed its mark.  We have edited the first paragraph to read:
	\begin{quote}
		Phylogeographic methods
	\cite{lemey2009bayesian, lemey2014unifying, holbrook2021massive,
	holbrook2022viral} model large-scale viral transmission between
human populations as a function of the shared evolutionary history of
the viral population of interest.  Data take the form of dates,
locations and genome sequences associated to individual viral
samples. Spatiotemporal structure interfaces with network structure
given by the phylogeny, or family tree, describing the viruses'
collective history beginning with the most recent common ancestor.
While one cannot directly observe this history, one may statistically
reconstruct the phylogenetic tree by positing that changes in the
viral genome happen randomly at regular intervals, thereby capturing
the intuition that viral samples with more differences between their
(aligned) sequences should find themselves further apart on the family
tree.
	\end{quote}
\end{reply}

* Figure 1 is designed to show how the approximation behaves beyond the assumptions of theorems and corollaries, and support the claim that the results apply more generally by numerical evidence. I am not convinced that this constitutes strong evidence (as it is claimed in the manuscript), however. My feeling is that the claim holds, but it needs to be demonstrated with distributions that are not so docile. I think that deviations from the assumptions of theorems and corollary are not so large for cases a, b, and c. For Figure 1c, I have a specific comment: I am not sure about what the authors mean by "Cauchy entries truncated to be positive", but my understanding is that this is a folded Cauchy distribution at zero with the support of the probability density function defined on the non-negative real line (if this is not correct please add a sentence to describe). Cauchy distribution is a member of the subexponential family and I wonder whether the tail properties of the folded Cauchy are distinct from those of the subexponential family (I do not have an answer to this, so I suggest that the authors check and provide a brief explanation). 

* Equations 12-19 seems to follow straightforwardly from the section where equation 2-7 are presented. I agree that writing them explicitly is better, but I felt that they disrupt the flow of the section "Deterministic bounds on approximation error in time". Please consider emulating them to somewhere in 2-7 or following, intermediate steps in 12-19 maybe put in an appendix. 

* In presenting the subexponential families in formulae 40-41, notation "exp" is used instead of "e". Please change to "e" to be consistent with the rest of the manuscript. Also, I think this definition comes from Vershynin. There might be some regularity conditions attached to this specific definition, although I cannot remember exactly. Please check. There is a more fundamental definition of subexponential families using the cumulative distribution functions, maybe that serves better here. Your choice of course. 

* Remark 6. The sentence starting with "In general..." is not clear in which sense it is general. "Proxy" is also not clear. 

* The paragraph starting with "In addition..." on page 7 is too short for the reader to understand the implications of the analysis. Please consider expanding it. 

* Appendix D. Surrogate-trajectory Hamiltonian Monte Carlo is described quite detached from the work presented in the manuscript. Please consider describing it in the context of the work. 

* For completeness, it is worth restating that Q is symmetric in Theorem 3. (Or is it not?) 

* Remark 2 (iii). "eigenvalue" should be "eigenvalues". 

* References (5) is a book, but there is no section or page.


%This paper presented an interesting framework for large-scale MCMC. It leverages the quantum speedup of Grover's searching algorithm to accelerate the treatment of multi-proposals in MCMC. However, an implementation of a quantum algorithm involves nontrivial steps, which need to be explained before this paper can be accepted.
%
%\begin{reply}
%	I thank the Reviewer for taking the time to review the manuscript and for their positive reception.
%\end{reply}
%
%The author did not clarify whether this is a fully quantum algorithm, or a hybrid one. It is important to explain what is the input of the quantum computer and what to extract. For the input, one has to encode it coherently into a quantum state $\ket{\psi}$, and estimate the complexity associated with such preparation.
%
%
%
%To extract quantities from a quantum computer, one has to express it as an expectation (observable), and estimate the probability of obtaining the correct output. 
%
%\begin{reply}
%	I have clarified the hybrid nature of the QPMCMC computing scheme.  The first two sentences of the abstract now read:
%	\begin{quote}
%		We propose a novel hybrid quantum computing strategy for parallel MCMC algorithms that generate multiple proposals at each step. This strategy makes the rate-limiting step within parallel MCMC amenable to quantum parallelization by using the Gumbel-max trick to turn the generalized accept-reject step into a discrete optimization problem.
%	\end{quote}
%
%	In response to the Reviewer's important points regarding on-loading and off-loading data to and from the quantum machine, I have added the step $\ket{\theta_p} \leftarrow \ttheta_p$ within Algorithm \ref{alg:qpMCMC}, along with the comment ``\verb|Load proposal onto quantum computer.|''. Algorithm 4, which is nested within Algorithm 6, already addresses extraction.  Perhaps more importantly, I have added the following paragraph to the Discussion (Section \ref{sec:disc}):
%	
%	\begin{quote}
%There are major technical barriers to the practical implementation of QPMCMC.  The framework, like other quantum machine learning (QML) schemes, requires on-loading and off-loading classical data to and from a quantum machine.  In the seminal review of QML, \citet{biamonte2017quantum} discuss what they call the `input problem'.  \citet{cortese2018loading} present a general purpose quantum circuit for loading $B$ classical bits into a quantum data structure comprising $\log_2 (B)$ qubits with circuit depth $\order{\log (B)}$.  For continuous targets, QPMCMC requires reading $\order{P}$ real vectors $\ttheta_p\in \mathbb{R}^D$ onto the quantum machine at each MCMC iteration.  If $b$ is the number of bits used to represent a single real value, then reading $B=\order{PDb}$ classical bits into the quantum machine requires time $\order{\log (PDb)}$.  This is true whether one opts for fixed-point \citep{jordan2005fast} or floating-point \citep{haener2018quantum} representations for real values.  The outlook can be better for discrete distributions.  For example, the QPMCMC scheme presented in Section \ref{sec:disc} only requires loading the entire configuration state once prior to sampling.  A $D$-spin Ising model requires $D$ classical bits to encode, and these bits load onto a quantum machine in time $\order{\log(D)}$.  Once one has encoded the entire system state, each QPMCMC iteration only requires loading the addresses of the $\order{P}$ proposal states. If one uses $b$ bits to encode each address, then the total time required to load data onto the quantum machine is $\order{\log(Pb)}$ for each QPMCMC iteration.  On the other hand, the speedup for discrete targets assumes the ability to hold an entire configuration in QRAM.
%Conveniently, the `output problem' is less of an issue for QPMCMC, as only a single integer $\hat{p} \in \{0,\dots,P\}$ need be extracted within Algorithm \ref{alg:min}.
%	\end{quote}
%\end{reply}
%
%There have already been numerous quantum algorithms for MCMC, e.g.,
%\begin{enumerate}
%	\item Montanaro, A. Quantum Speedup of Monte Carlo Methods. Proc. R. Soc. A. 2015, 471 (2181), 20150301. \citep{montanaro}
%	
%	\item Wocjan, P.; Abeyesinghe, A. Speedup via Quantum Sampling. Phys. Rev. A 2008, 78 (4), 042336. \citep{wocjan2008speedup}
%\end{enumerate}
%The author should comment on how the present approach would compare to these existing methods.
%
%\begin{reply}
%I thank the Reviewer for the helpful suggestion.  To clarify the relationship between the proposed method and other quantum Monte Carlo algorithm, I have included the following paragraph in Section \ref{sec:disc}:
%\begin{quote}
%Second, can QPMCMC be combined with established quantum algorithms that make use of `quantum walks' on graphs to sample from discrete target distributions?  \citet{szegedy2004quantum} presents a quantum analog to classical ergodic reversible Markov chains and shows that such quantum walks sometimes provide quadratic speedups over classical Markov chains.  \citet{szegedy2004quantum} also points out that Grover's search, a key component within QPMCMC, may be interpreted as performing just such a quantum walk on a complete graph.  \citet{wocjan2008speedup} accelerate the quantum walk by using ancillary Markov chains to improve mixing and apply their method to simulated annealing.  Given a quantum algorithm $\mathbb{A}$ for producing a discrete random sample with variance $\sigma^2$, \citet{montanaro} develops a quantum algorithm for estimating the mean of algorithm $\mathbb{A}$'s output with error $\epsilon$ by running algorithm $\mathbb{A}$ a mere $\widetilde{\mathcal{O}}(\sigma/\epsilon)$ times, where $\widetilde{\mathcal{O}}$ hides polylogarithmic terms. Importantly, this quadratic quantum speedup over classical Monte Carlo applies for quantum algorithms $\mathbb{A}$ that only feature a single measurement such as certain simple quantum walk algorithms. Unfortunately, this assumption fails for quantizations of Metropolis-Hastings, in general, and QPMCMC, in particular. More promising for QPMCMC, \citet{lemieux2020efficient} develop a quantum circuit that applies Metropolis-Hastings to the Ising model without the need for oracle calls.  An interesting question is whether similar non-oracular quantum circuits exist for the basic parallel MCMC backbone to QPMCMC.  In general, however, comparison between QPMCMC and other quantum Monte Carlo techniques is challenging because the foregoing literature (\textcolor{red}{a}) largely focuses on MCMC as a tool for discrete optimization, with algorithms that only ever return a single Monte Carlo sample or function thereof, and (\textcolor{red}{b}) restricts itself to a handfull of simple, stylized and discrete target distributions.  On the other hand, QPMCMC is a general inferential framework for sampling from general discrete and continuous distributions alike.
%\end{quote}
%\end{reply}
%
%\begin{reply}
%	Again, I'd like to thank the Reviewer for suggesting the above additions/edits, and I believe the manuscript is now much improved as a result.
%\end{reply}
%
%
%\section*{Reviewer 2}
%
%
%This paper seeks to further develop parallel Markov chain Monte Carlo in the quantum computing hardware. The topic is very interesting. The author has a nice comprehensive review of multi-proposal parallel MCMC, and is very current in literature. The review of established quantum computing algorithms is also very helpful for statistician who might have limited knowledge in the area. 
%
%The key contribution is connecting parallel Markov chain Monte Carlo to the quantum computing. The key step is to employ Gumbel-max trick to transform the generalized accept-reject step (i.e., Dirichlet sampling) into a discrete optimization procedure, so that quantum optimization algorithm can be used for sampling. The idea is simple and elegant. It just connects the dots.
%
%\begin{reply}
%	I thank the Reviewer for their kind words and for taking the time to read the manuscript.
%\end{reply}
%
%I still have a few concerns, more generally related to the “bedrock” parallel Markov chain Monte Carlo:
%
%\begin{enumerate}
%	\item Although named as parallel MCMC, the algorithm is still MCMC, which relies on the back-bone Markov chain to achieve equilibrium. The algorithm still heavily relies on the long sequence for the Markov chain to reach equilibrium. The quantum part doesn’t seem to be able to solve this major drawback of parallel MCMC. 
%\end{enumerate}	
%
%\begin{reply}
%	I absolutely agree with the Reviewer's judgment.   To clarify that my method does not overcome this basic shortcoming of MCMC, I've included the following text in Section \ref{sec:disc}:
%	\begin{quote}
%		While the algorithm still must construct long Markov chains to reach equilibrium, generating each individual Markov chain state requires significantly fewer target evaluations.
%	\end{quote} 
%\end{reply}
%	
%	
%\begin{enumerate}
%	  \setcounter{enumi}{1}
%	\item I see more potential of the Gumbel-max trick $+$ quantum computing in sequential Monte Carlo. Unlike MCMC, SMC only requires a handful of sequential chains, and mostly need to have a very large number of proposals for re-sampling, where your trick could have more impact.
%\end{enumerate}
%
%\begin{reply}
%	This is a good point.  I've added the following to the second-to-last paragraph of Section \ref{sec:disc}:
%	\begin{quote}
%		The trick may also find use within sequential Monte Carlo \citep{doucet2001sequential}.  For example, \citet{berzuini2001resample} present a sequential importance resampling algorithm that uses MCMC-type moves to encourage particle diversity and avoid the need for bootstrap resampling.  Multiproposals accelerated by the quantum Gumbel-max trick could add speed and robustness to such MCMC-resampling.
%	\end{quote}	
%\end{reply}
%
%\begin{enumerate}
%	  \setcounter{enumi}{2}
%	\item A last comment is on computer hardware structure. In the QPMCMC, the Markov chain for loop requires CPU, the evaluation of log-likelihood requires GPU, and the sample from Dirichlet requires quantum processor. Will there be I/O communication overhead for these steps on distinct processors?
%\end{enumerate}
%
%\begin{reply}
%	I thank the Reviewer for raising this possible point of confusion.  The multiproposal is generated on a CPU or GPU.  The quantum computer both (a) evaluates the log-posterior and (b) samples from the Dirichlet.  This is the nice thing about combining the Gumbel-max trick with quantum optimization.  The question about I/O is important, and I have added the following paragraph to Section \ref{sec:disc}:
%	\begin{quote}
%There are major technical barriers to the practical implementation of QPMCMC.  The framework, like other quantum machine learning (QML) schemes, requires on-loading and off-loading classical data to and from a quantum machine.  In the seminal review of QML, \citet{biamonte2017quantum} discuss what they call the `input problem'.  \citet{cortese2018loading} present a general purpose quantum circuit for loading $B$ classical bits into a quantum data structure comprising $\log_2 (B)$ qubits with circuit depth $\order{\log (B)}$.  For continuous targets, QPMCMC requires reading $\order{P}$ real vectors $\ttheta_p\in \mathbb{R}^D$ onto the quantum machine at each MCMC iteration.  If $b$ is the number of bits used to represent a single real value, then reading $B=\order{PDb}$ classical bits into the quantum machine requires time $\order{\log (PDb)}$.  This is true whether one opts for fixed-point \citep{jordan2005fast} or floating-point \citep{haener2018quantum} representations for real values.  The outlook can be better for discrete distributions.  For example, the QPMCMC scheme presented in Section \ref{sec:disc} only requires loading the entire configuration state once prior to sampling.  A $D$-spin Ising model requires $D$ classical bits to encode, and these bits load onto a quantum machine in time $\order{\log(D)}$.  Once one has encoded the entire system state, each QPMCMC iteration only requires loading the addresses of the $\order{P}$ proposal states. If one uses $b$ bits to encode each address, then the total time required to load data onto the quantum machine is $\order{\log(Pb)}$ for each QPMCMC iteration.  On the other hand, the speedup for discrete targets assumes the ability to hold an entire configuration in QRAM.
%Conveniently, the `output problem' is less of an issue for QPMCMC, as only a single integer $\hat{p} \in \{0,\dots,P\}$ need be extracted within Algorithm \ref{alg:min}.
%	\end{quote}
%\end{reply}
%
%
%\begin{reply}
%	Again, I'd like to thank the Reviewer for taking the time to review this manuscript and for their positive reception.  I hope that my edits have addressed their concerns.
%\end{reply}
%
%
%
%\section*{Associate Editor}
%
%
%
%The author proposes a quantum parallel MCMC algorithm that uses a Gumbel-max trick to quantum parallelize a multiple-proposal MCMC algorithm. The resulting algorithm drastically reduces the number evaluations of the target density. The manuscript is quite well written and motivated.
%
%\begin{reply}
%	I thank the AE for their generally positive reception!
%\end{reply}
%
%
%The referees are relatively positive about the work, however, certain improvements are deemed necessary and the points raised by them are important. I highlight a few points here, and have some additional comments.
%
%
%The first referee highlights that more specific details about the quantum algorithm are required. The second referee asks an important question regarding the cost of the I/O communication. I would further add that in the displayed Algorithms, it would be useful to indicate which components are CPU parallelized, GPU parallelized, and quantum parallelized.
%
%\begin{reply}
%I completely agree with the Reviewers' points and have added the following paragraph describing the burden associated with loading classical data onto a quantum machine:
%\begin{quote}
%There are major technical barriers to the practical implementation of QPMCMC.  The framework, like other quantum machine learning (QML) schemes, requires on-loading and off-loading classical data to and from a quantum machine.  In the seminal review of QML, \citet{biamonte2017quantum} discuss what they call the `input problem'.  \citet{cortese2018loading} present a general purpose quantum circuit for loading $B$ classical bits into a quantum data structure comprising $\log_2 (B)$ qubits with circuit depth $\order{\log (B)}$.  For continuous targets, QPMCMC requires reading $\order{P}$ real vectors $\ttheta_p\in \mathbb{R}^D$ onto the quantum machine at each MCMC iteration.  If $b$ is the number of bits used to represent a single real value, then reading $B=\order{PDb}$ classical bits into the quantum machine requires time $\order{\log (PDb)}$.  This is true whether one opts for fixed-point \citep{jordan2005fast} or floating-point \citep{haener2018quantum} representations for real values.  The outlook can be better for discrete distributions.  For example, the QPMCMC scheme presented in Section \ref{sec:disc} only requires loading the entire configuration state once prior to sampling.  A $D$-spin Ising model requires $D$ classical bits to encode, and these bits load onto a quantum machine in time $\order{\log(D)}$.  Once one has encoded the entire system state, each QPMCMC iteration only requires loading the addresses of the $\order{P}$ proposal states. If one uses $b$ bits to encode each address, then the total time required to load data onto the quantum machine is $\order{\log(Pb)}$ for each QPMCMC iteration.  On the other hand, the speedup for discrete targets assumes the ability to hold an entire configuration in QRAM.
%Conveniently, the `output problem' is less of an issue for QPMCMC, as only a single integer $\hat{p} \in \{0,\dots,P\}$ need be extracted within Algorithm \ref{alg:min}.
%\end{quote}
%I have also expanded the third paragraph of Section \ref{sec:intro} to help clarify the nature of parallelization within the proposed framework:
%\begin{quote}
%	One need not use parallel computing to implement Algorithm \ref{alg:pMCMC}, but the real promise and power of parallel MCMC comes from its natural parallelizability \citep{calderhead2014general}. 
%	Contemporary hardware design emphasizes architectures that enable execution of multiple mathematical operations simultaneously. Parallel MCMC techniques stand to leverage technological developments that keep modern computation on track with Moore's Law, which predicts that processing power doubles every two years.  For example, the algorithm of \citet{tjelmeland2004using} generates $P$ conditionally independent proposals and then evaluates the probabilities of \eqref{eq:probs}.  One may parallelize the proposal generation step using parallel pseudorandom number generators (PRNG) such as those advanced in \citet{salmon2011parallel}. The computational complexity of the target evaluations $\pi(\ttheta_p)$ is linear in the number of proposals. This presents a significant burden when $\pi(\cdot)$ is computationally expensive, e.g., in big data settings or for Bayesian inversion, but evaluation of the target density over the $P$ proposals is again a naturally parallelizable task.  Moreover, widely available machine learning software such as \textsc{TensorFlow} allows users to easily parallelize both random number generation and target evaluations on general purpose graphics processing units (GPU) \citep{lao2020tfp}. Finally, when generating independent proposals using a proposal distribution of the form $q(\ttheta_0,\Ttheta_{-0})=\prod_{p=1}^Pq(\ttheta_0,\ttheta_{p})$, the acceptance probabilities \eqref{eq:probs} require the $\order{P^2}$ evaluation of the $P+1\choose 2$ terms $q(\ttheta_{p},\ttheta_{p'})$, but \citet{massive,holbrook2021scalable} demonstrate the natural parallelizability of such pairwise operations, obtaining orders-of-magnitude speedups with contemporary GPUs.  The proposed method directly addresses the acceptance step of Algorithm \ref{alg:pMCMC}, while leaving the choice of parallelizing (or not parallelizing) the proposal step to the practitioner.  
%\end{quote}
%\end{reply}
%
%
%The author has motivated the need for quantum algorithms for big-data problems where evaluation of the target is expensive. However, only data-less examples have been implemented. Implementation on a real example is warranted. Further, comparisons with other quantum methods (as suggested by referee) would be useful to understand the effectiveness of the proposed methodology. Finally, comparison with the non-quantum version of the multiple-proposal would be helpful to make the point that the mixing of the algorithm remains unaffected.
%
%\setcounter{figure}{7}
% \begin{figure}[!t]
%	\centering
%	\includegraphics[width=0.9\linewidth]{figures/blackHole.png}
%	\caption{On the left is a 4,076-by-4,076 intensity map of the shadow of supermassive black hole Sagittarius A*  \citep{akiyama2022first}.  On the right is the pixelwise posterior mode of a Bayesian image classification model fit to intensity data.  Within a Metropolis-in-Gibbs routine, quantum parallel MCMC using 1,024 proposals requires less that one-tenth the posterior evaluations required by conventional parallel MCMC.}\label{fig:blackHole}
%\end{figure}
%
%\begin{reply}
%	I thank the AE for these helpful suggestions. In response, I have added Section \ref{sec:bayesIm}, in which I apply a Bayesian image classification model to a recently released 4000$\times$4000 image of the supermassive black hole Sag A*.  For this target, QPMCMC scores a 10-fold speedup over parallel MCMC:
%	\begin{quote}
%		We apply a Bayesian image classification model to an intensity map (Figure \ref{fig:blackHole}) of the newly imaged supermassive black hole, Sagittarius A*, located at the Galactic Center of the Milky Way \citep{akiyama2022first}.  Whereas one cannot see the black hole itself, one may see the shadow of the black hole cast by the hot, swirling cloud of gas surrounding it.  We extract the black hole from the surrounding cloud by modeling the intensity at each of the $D=$4,076$^2=$16,613,776 pixels as belonging to a mixture of two truncated normals with values $y_{d}$ restricted to the intensity range $[0,255]$.  Namely, we follow \citet{hurn1997difficulties} and specify the latent variable model
%		\begin{align*}
%			y_d |(\mu_\ell, \sigma^2, \theta_d) &\stackrel{ind}{\sim} \mbox{Normal}(\mu_\ell,\sigma^2 ) \, ,\quad y_d \in [0,255] \, , \quad \theta_d=\ell \,,\quad d\in \{1,\dots,D\}\, ,\\
%			\mu_\ell &\stackrel{iid}{\sim} \mbox{Uniform} (0,255) \,  , \quad \ell \in \{-1,1\} \, , \\
%			\frac{1}{\sigma^2} &\sim  \mbox{Gamma}\left(\frac{1}{2}, \frac{1}{2} \right)  \, ,
%		\end{align*}
%		where $\ttheta=(\theta_1,\dots,\theta_D)$ share for a prior the Ising model \eqref{eq:ising} with edges joining neighboring pixels and interaction $\rho=1.2$. 
%		
%		We use a QPMCMC-within-Gibbs scheme to infer the join posterior of $\ttheta$ and the three mixture model parameters.  For the former, we use the same QPMCMC scheme as in Section \ref{sec:ising} with 1,024 proposals at each iteration. For the latter, we use the closed-form updates presented in \citet{hurn1997difficulties}.  We run this scheme for 20 million iterations, discarding the first 10 million as burnin.  We thin the remaining sample at a ratio of 1 to 40,000 for the latent memberships $\ttheta$ and 1 to 4,000 for the three parameters $\mu_{-1}$, $\mu_1$ and $\sigma^2$.  Using the \textsc{R} package \textsc{coda} \citep{coda}, we calculate effective sample sizes of the log-posterior (257.1), $\mu_{-1}$ (1,578.7), $\mu_1$ (257.6) and $\sigma^2$ (2,500.0), suggesting adequate convergence.   Figure \ref{fig:blackHole} shows both the intensity data and the pixelwise posterior mode of the latent membership vector $\ttheta$.  The QPMCMC requires only 1,977,553,608 target evaluations compared to the 1,024 $\times$ 20,000,000 $=$ 2.048$\times10^{10}$ evaluations required for the analogous parallel MCMC scheme implemented on a conventional computer, representing a 10.36-fold speedup.
%	\end{quote}
%	I have also added the following paragraph to Section \ref{sec:disc} comparing QPMCMC with other quantum methodologies:
%	\begin{quote}
%Second, can QPMCMC be combined with established quantum algorithms that make use of `quantum walks' on graphs to sample from discrete target distributions?  \citet{szegedy2004quantum} presents a quantum analog to classical ergodic reversible Markov chains and shows that such quantum walks sometimes provide quadratic speedups over classical Markov chains.  \citet{szegedy2004quantum} also points out that Grover's search, a key component within QPMCMC, may be interpreted as performing just such a quantum walk on a complete graph.  \citet{wocjan2008speedup} accelerate the quantum walk by using ancillary Markov chains to improve mixing and apply their method to simulated annealing.  Given a quantum algorithm $\mathbb{A}$ for producing a discrete random sample with variance $\sigma^2$, \citet{montanaro} develops a quantum algorithm for estimating the mean of algorithm $\mathbb{A}$'s output with error $\epsilon$ by running algorithm $\mathbb{A}$ a mere $\widetilde{\mathcal{O}}(\sigma/\epsilon)$ times, where $\widetilde{\mathcal{O}}$ hides polylogarithmic terms. Importantly, this quadratic quantum speedup over classical Monte Carlo applies for quantum algorithms $\mathbb{A}$ that only feature a single measurement such as certain simple quantum walk algorithms. Unfortunately, this assumption fails for quantizations of Metropolis-Hastings, in general, and QPMCMC, in particular. More promising for QPMCMC, \citet{lemieux2020efficient} develop a quantum circuit that applies Metropolis-Hastings to the Ising model without the need for oracle calls.  An interesting question is whether similar non-oracular quantum circuits exist for the basic parallel MCMC backbone to QPMCMC.  In general, however, comparison between QPMCMC and other quantum Monte Carlo techniques is challenging because the foregoing literature (\textcolor{red}{a}) largely focuses on MCMC as a tool for discrete optimization, with algorithms that only ever return a single Monte Carlo sample or function thereof, and (\textcolor{red}{b}) restricts itself to a handfull of simple, stylized and discrete target distributions.  On the other hand, QPMCMC is a general inferential framework for sampling from general discrete and continuous distributions alike.
%	\end{quote}
%Finally, I've added Section \ref{sec:mixing}, which compares mixing between conventional parallel MCMC and QPMCMC:
%\begin{quote}
%	To ascertain whether QPMCMC mixes differently compared to conventional parallel MCMC, we run both algorithms for a range of proposal counts to sample from a 10-dimensional standard normal distribution. For each algorithm and proposal setting, we run 100 independent chains for 10,000 iterations and obtain effective sample sizes ESS$_d$ for $d \in \{1,\dots,10\}$.   We then calculate the relative differences between the means and minima of one chain generated using parallel MCMC and one chain generated using QPMCMC; for example:
%	\begin{align*}
%		\mbox{Relative difference between means} \: \overline{\mbox{ESS}}_{(\cdot)} = \frac{\left|\overline{\mbox{ESS}}_{pMCMC} - \overline{\mbox{ESS}}_{QPMCMC}\right| }{\overline{\mbox{ESS}}_{pMCMC} }
%	\end{align*}
%	Figure \ref{fig:mixing} shows results.  In general, the majority relative differences are small.  For both statistics, mean relative differences are less than 0.05, regardless of proposal count.  Again for both statistics, more than 75\% of the independent runs result in relative differences below 0.1.  We note that relative differences between means (blue) appear to decrease with the number of proposals, but the same does not hold for relative differences between minima (red).
%\end{quote}
%\end{reply}
%
% \begin{figure}[!t]
%	\centering
%	\includegraphics[width=0.7\linewidth]{figures/pMCMCvsQPMCMC.pdf}
%	\caption{Relative differences between effective sample sizes (ESS) for parallel MCMC (pMCMC) and quantum parallel MCMC (QPMCMC) across a range of proposal counts. We target a 10-dimensional standard normal distribution.  For each algorithm and each proposal setting, we generate 100 independent chains of length 10,000 and calculate the mean and minimum ESS across dimensions.}\label{fig:mixing} 
%\end{figure}
%
%
%\subsection*{Additional Comments}
%
%
%\begin{itemize}
%	\item (I am assuming the following is true.) It would be helpful to state clearly at some point that the resulting QPMCMC chain has the same transition kernel as the non-quantum version. That is, it remains exact.
%\end{itemize}	
%
%\begin{reply}
%	Sometimes Algorithm \ref{alg:min} does not find the minimum. For example, in the experiments described in Figures \ref{fig:mcmcIterations} and \ref{fig:qq}, the correct kernel is used 99.4\% of the time.  Because of this the QPMCMC chain does not have the same transition kernel as the non-quantum version once every 200 QPMCMC iterations.  Nonetheless, results like those in Figure \ref{fig:qq} show that bias remains small.  The second paragraph of Section \ref{sec:disc} reads:
%	\begin{quote}
%This work leads to three interesting questions.   First, what is the status of inference obtained by QPMCMC?  The QPMCMC selection step relies on quantum minimization, an algorithm that only achieves success with probability $1-\epsilon$.  While empirical studies suggest that this error induces little bias, it would be helpful to use this $\epsilon$ to bound the distance between the target distribution and the distribution generated by QPMCMC.  Such theoretical efforts would need to extend recent formalizations of the multiproposal based parallel MCMC paradigm \citep{glatt} to account for biased MCMC kernels.
%	\end{quote}
%I am currently working with collaborators to bound the Wasserstein distance between the target and the distribution actually sampled.  At the moment, I must leave such results for future work. 
%\end{reply}
%	
%\begin{itemize}	
%	\item Equation (1): It is not clear to me how the ``a.s.'' applies to this definition of 1
%	stationarity.
%\end{itemize}
%
%\begin{reply}
%	I thank the AE for catching this silly mistake, which  I have fixed.
%\end{reply}
%
%\begin{itemize}
%	
%	\item Algorithm 1: The algorithm of Tjelmeland (2004) is described here as ``Parallel MCMC'', but it is not immediately clear which part of the algorithm will be implemented in a parallelizable way. I would recommend changing the caption.
%	
%\end{itemize}
%
%\begin{reply}
%	This caption uses the term `parallel MCMC' in the same way that it is used in \citet{calderhead2014general}, \citet{schwedes2019parallel}, \citet{schwedes2021rao} and \citet{glatt}, i.e., to indicate the use of multiple proposals.  To help resolve the potential confusion, I have changed the caption to `Parallel (multiproposal) MCMC' and added the following text to Section \ref{sec:intro}:  
%	\begin{quote}
%		One need not use parallel computing to implement Algorithm \ref{alg:pMCMC}, but the real promise and power of parallel MCMC comes from its natural parallelizability \citep{calderhead2014general}. 
%	\end{quote}
%\end{reply}
%
%\begin{itemize}
%
%	\item page 4, line 10: I believe the citation for \citet{gelman1992inference} is incorrect. Did the author mean \citet{gelman1992single}?
%
%\end{itemize}
%
%\begin{reply}
%	I really did mean \citet{gelman1992inference}, but I have now changed the text to cite both papers following the AE's helpful recommendation.  All I'm saying here is that there are different kinds of parallelization available, and that computational statisticians should engage the entire spectrum.  
%\end{reply}
%
%\begin{itemize}
%	
%	\item Algorithm 5: Should the last step be ``return $\hat{p}$''.
%	
%\end{itemize}
%
%\begin{reply}
%	I thank the AE for catching this, and I have changed the text accordingly.
%\end{reply}
%
%\begin{itemize}
%	
%	\item Section 3.1: The proof of the Gumbel-max trick can likely be either removed or
%	moved to the Appendix, since this is a fairly well-known result.
%
%\end{itemize}
%
%\begin{reply}
%	I have moved the proof to Section \ref{sec:gm} of the Appendix.
%\end{reply}
%
%\begin{itemize}	
%	
%	\item Algorithm 6: I would recommend explicitly highlighting which parts of this algorithm are quantum parallelized.
%
%\end{itemize}
%
%\begin{reply}
%	I have highlighted the lines accordingly.  Also, I have edited the relevant text to read:
%	\begin{quote}
%		Algorithm \ref{alg:qpMCMC} presents the details of QPMCMC for a continuously-valued target.  The algorithm uses conventional (perhaps parallel) computing almost entirely, excluding two lines that are highlighted.  The first of these loads proposals onto the quantum machine, and the second line that calls the quantum minimization algorithm presented in Algorithm \ref{alg:min}. 
%	\end{quote}
%\end{reply}
%
%\begin{itemize}
%	
%	\item Figure 5: The caption here is confusing. It reads ``Total number of oracle evaluations required for each of 2000 parallel MCMC iterations''. Traditionally, this would imply there were 2000 independent chains runs. However, the text describing this Figure implies that there were 2000 Gaussian proposals.
%	
%\end{itemize}
%
%\begin{reply}
%	I thank the AE for pointing out this error on my part.  In fact, the first line should have included `quantum parallel MCMC' instead of just `parallel MCMC'.
%	
%	\begin{quote}
%		Total number of oracle evaluations required for each of 2,000 quantum parallel MCMC (QPMCMC) iterations for standard multivariate normal targets of five different dimensionalities. Regardless of target dimension, the individual QPMCMC runs require roughly $7$\% of the usual 4 million target evaluations. Over  99.4\% of the 10,000 MCMC iterations across all dimensions successfully sample from the discrete distribution with probabilities of \eqref{eq:probs}. Burn-in iterations require moderately more evaluations because the current state occupies a lower density region and represents a `less good' warm-start.
%	\end{quote}
%\end{reply}
%
%\begin{itemize}
%	
%	\item page 22: The authors tune to 50\% acceptance rate, but I believe the optimal in high- dimensions from the reference \citet{rosenthal2011optimal} is 23.4\%. Further, the tuning to 23.4\% acceptance rate is strictly for the Metropolis-Hastings acceptance function. In the multiple-proposal world, the acceptance function employed is completely different, implying different optimality results. See \citet{agrawal2021optimal}.
%\end{itemize}
%
%\begin{reply}
%	I completely agree with the AE on this one.  The reference to \citet{rosenthal2011optimal} was for the `standard adaptation procedure', and not the 50\% tuning target.  I use this target because it is close to the roughly 60\% target I found optimal in my paper about another multiproposal technique \citep{holbrook2021generating} and because there are no theoretical optimality results in the multiproposal setting that I am aware of.  I've edited the text to make this clear:
%	\begin{quote}
%		We fix the number of Gaussian proposals to be 2,000 and use standard adaptation procedures \citep{rosenthal2011optimal} to target a 50\% acceptance rate while guaranteeing convergence. Although no theoretical results validate this target acceptance rate, the rate is close to empirically optimal rates from \citet{holbrook2021generating}.
%	\end{quote}
%\end{reply}
%
%\begin{itemize}	
%	
%	\item Figure 5: ``Over 99.4\% of the 10,000 MCMC iterations successfully sampled from the discrete distribution with probabilities of Equation (2)''. It’s unclear exactly how this number comes about and what it means. Once the quantum minimization returns a $\hat{p}$, the authors have shown that it follows the right distribution. How can then (1) .6\% not be from the right distribution, and (2) how does one even quantify this.
%\end{itemize}
%
%\begin{reply}
%For that and other related examples, I found that roughly 1 in 200 implementations of Algorithm 4 failed to find the minimum, i.e., failed to use the theoretically justified kernel.  We can make this failure happen less often by running Algorithm 4 longer, but there is a tradeoff between accuracy and target evaluations. Empirical results like those of Figure \ref{fig:qq} show that bias is relatively small, but I am currently working with collaborators to better understand how intermittent failure leads to biased sampling.  
%
%I quantify these things by running Monte Carlo simulations of all nested quantum algorithms beginning with the Grover iteration as basic building block.  This is implemented in R as described in Section \ref{sec:sim}.
%\end{reply}
%
%\begin{itemize}	
%	\item Figure 6: Considering the target is known, burn-in seems incredibly wasteful. Once can just start from stationarity and remove the need to throw away samples.
%\end{itemize}
%
%\begin{reply}
%	I include burnin here in order to demonstrate that warm-starting Algorithm 4 with the current Markov chain state becomes more beneficial as one converges the the target's high density regions. This explains why more target evaluations are required earlier on in the trajectory.
%\end{reply}
%
%\begin{itemize}	
%	\item Table 1: In producing this table, what effective sample size is being used for this two-dimensional problem?
%\end{itemize}
%
%\begin{reply}
%	I thank the AE for pointing out this omission. Here, I am using the minimum ESS between the two dimensions.  I  have edited the first sentence of the caption to say:
%	\begin{quote}
%		Racing to a minimum (between the two dimensions) of 100 effective samples for a target with 1,000 disjoint modes. 
%	\end{quote}
% In case this was a software question, I have also added information on ESS to the first paragraph of Section \ref{sec:sim}:
% \begin{quote}
%We use \textsc{R} \citep{rlang}, \textsc{Python} \citep{vanrossum1995python}, \textsc{TensorFlow} \citep{abadi2016tensorflow} and  \textsc{TensorFlow Probability MCMC} \citep{lao2020tfp} in our simulations and the \textsc{ggplot2} package to generate all figures \citep{ggplot}.   In \textsc{R}, we use the package \textsc{coda} \citep{coda} to calculate effective sample sizes.  In \textsc{Python}, we use a built-in function from \textsc{TensorFlow Probability MCMC}.  The primary color palette comes from \citet{wes}.
% \end{quote}
%\end{reply}
%
%\begin{itemize}	
%	\item Could the authors comment on the potential applicability of the algorithm on discrete state spaces? This may be useful for Bayesian record linkage, and variable selection problems \citep{zanella2020informed}.
%\end{itemize}
%
%\begin{reply}
%	This is a great question. I thank the AE for inspiring some additions to this manuscript.  I have added experiments in which I used QPMCMC to sample from an Ising model and a Bayesian classification model.  Figure \ref{fig:ising2d} shows benefits for using many proposals within QPMCMC and that speedups increase with the number of proposals!
%\end{reply}
%
%\setcounter{figure}{6}
% \begin{figure}[!t]
%	\centering
%	\includegraphics[width=0.7\linewidth]{figures/Ising2dFig.pdf}
%	\caption{Convergence of log-posterior for a parallel MCMC sampler targeting an Ising model with uniform interaction $\rho=1$ and no external field. All chains begin at minimal probability `checkerboard' state. Larger proposal counts allow discovery of higher probability configurations.}\label{fig:ising2d}
%\end{figure}
%
%\begin{reply}
%	Again, I thank the AE for all their helpful suggestions, and I am confident that the manuscript is now significantly improved as a result.
%\end{reply}



%\clearpage
\bibliographystyle{sysbio}
\bibliography{refs}


\end{document}
